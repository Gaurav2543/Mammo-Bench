{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pydicom\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_save_images(data):\n",
    "    j = 0\n",
    "    dataset_name = data['dataset'].iloc[0]  # Extract the dataset name from the first row\n",
    "    print(f\"Processing {dataset_name} dataset...\")\n",
    "    new_dir = f\"Original_Dataset/{dataset_name}\"\n",
    "\n",
    "    # Create a new directory to save the renamed images\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    print(f\"Saving images to {new_dir}...\")\n",
    "    new_paths = []\n",
    "\n",
    "    for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        old_path = row['original_image_path']\n",
    "        new_filename = f\"{row['dataset']}_{j}.jpg\"\n",
    "        new_path = os.path.join(new_dir, new_filename)\n",
    "        \n",
    "        # Save image based on file type\n",
    "        if old_path.endswith('.dcm'):\n",
    "            ds = pydicom.dcmread(old_path)\n",
    "            # img = ds.pixel_array\n",
    "            # plt.imsave(new_path, img, cmap='gray')\n",
    "\n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            new_paths.append(new_path)\n",
    "            j += 1\n",
    "        elif old_path.endswith('.png') or old_path.endswith('.jpeg') or old_path.endswith('.pgm'):\n",
    "            # img = mpimg.imread(old_path)\n",
    "            # plt.imsave(new_path, img, cmap='gray')\n",
    "            \n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            new_paths.append(new_path)\n",
    "            j += 1\n",
    "        else:\n",
    "            # Ensure the new path is unique\n",
    "            while os.path.exists(new_path):\n",
    "                j += 1\n",
    "                new_filename = f\"{row['dataset']}_{j}.jpg\"\n",
    "                new_path = os.path.join(new_dir, new_filename)\n",
    "            \n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            new_paths.append(new_path)\n",
    "\n",
    "            # Copy the file to the new path\n",
    "            shutil.copy(old_path, new_path)\n",
    "            j += 1\n",
    "    \n",
    "    print(f\"Total images saved: {j}\")\n",
    "\n",
    "    # remove the ../Individual_Original_Datasets/ from the original_image_path\n",
    "    data['original_image_path'] = data['original_image_path'].replace('../Individual_Original_Datasets/', '', regex=True)\n",
    "    # add mask_path column and preprocessed_image_path column\n",
    "    data['mask_path'] = data['new_path'].str.replace('Original_Dataset', 'Masks')\n",
    "    data['preprocessed_image_path'] = data['new_path'].str.replace('Original_Dataset', 'Preprocessed_Dataset')\n",
    "\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data.to_csv(f\"{dataset_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_and_duplicate_files(data):\n",
    "    # Directory containing the renamed images\n",
    "    new_dir = f\"Original_Dataset/{data.iloc[0]['dataset']}\"\n",
    "\n",
    "    # Generate a list of expected filenames\n",
    "    expected_filenames = [f\"{data.iloc[0]['dataset']}_{i}.jpg\" for i in range(len(data))]\n",
    "\n",
    "    # Generate a list of actual filenames in the directory\n",
    "    actual_filenames = sorted(os.listdir(new_dir))\n",
    "\n",
    "    # Find missing filenames\n",
    "    missing_filenames = set(expected_filenames) - set(actual_filenames)\n",
    "\n",
    "    # Find duplicate filenames\n",
    "    duplicate_filenames = [filename for filename in actual_filenames if actual_filenames.count(filename) > 1]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Total expected filenames: {len(expected_filenames)}\")\n",
    "    print(f\"Total actual filenames: {len(actual_filenames)}\")\n",
    "    print(f\"Missing filenames: {len(missing_filenames)}\")\n",
    "    print(f\"Duplicate filenames: {len(duplicate_filenames)}\")\n",
    "\n",
    "    if missing_filenames:\n",
    "        print(\"Missing filenames:\")\n",
    "        for filename in sorted(missing_filenames):\n",
    "            print(filename)\n",
    "\n",
    "    if duplicate_filenames:\n",
    "        print(\"Duplicate filenames:\")\n",
    "        for filename in sorted(set(duplicate_filenames)):\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mias = pd.read_csv('../Individual_Original_Datasets/MIAS/Info.txt', sep=' ')\n",
    "\n",
    "# drop the column Unnamed: 7\n",
    "mias = mias.drop(columns=['Unnamed: 7'])\n",
    "mias = mias.rename(columns={'CLASS': 'class', 'SEVERITY': 'classification', 'REFNUM': 'patientID', 'BG': 'density', 'RADIUS': 'radius', 'X': 'x', 'Y': 'y'})\n",
    "mias['classification'] = mias['classification'].replace('B', 'Benign')\n",
    "mias['classification'] = mias['classification'].replace('M', 'Malignant')\n",
    "mias['density'] = mias['density'].replace('D', 'Dense')\n",
    "mias['density'] = mias['density'].replace('F', 'Fatty')\n",
    "mias['density'] = mias['density'].replace('G', 'Glandular')\n",
    "mias['image_path'] = 'MIAS/all-mias/' + mias['patientID'].astype(str) + '.pgm'\n",
    "mias['image_type'] = 'full mammogram image'\n",
    "mias['dataset'] = 'mias'\n",
    "\n",
    "mias['image_path'] = mias['image_path'].str.replace('MIAS/all-mias/', '../Individual_Original_Datasets/MIAS/all-mias/')\n",
    "# remove rows with NaN values in image_path\n",
    "mias = mias.dropna(subset=['image_path'])\n",
    "\n",
    "# remove duplicate rows\n",
    "mias = mias.drop_duplicates(subset=['image_path'])\n",
    "\n",
    "#  change the name of the column image path to original_image_path\n",
    "mias = mias.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "print(mias.shape)\n",
    "print(mias.count())\n",
    "print(mias.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(mias)\n",
    "find_missing_and_duplicate_files(mias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INbreast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess inbreast\n",
    "inbreast = pd.read_excel('../Individual_Original_Datasets/INbreast/INbreast.xls')\n",
    "\n",
    "# drop the entry with empty file name\n",
    "inbreast = inbreast.dropna(subset=['File Name'])\n",
    "inbreast['ACR'] = inbreast['ACR'].replace(\"'\", np.nan)\n",
    "inbreast['density'] = inbreast['ACR']\n",
    "\n",
    "inbreast = inbreast.drop(columns=['Patient age', 'Pectoral Muscle Annotation', 'Acquisition date', 'Mass ', 'Micros', 'Distortion', 'Asymmetry', 'Lesion Annotation Status', 'Patient ID', 'Findings Notes (in Portuguese)', 'Other Notes', 'Other Annotations', 'Acquisition date'])\n",
    "inbreast = inbreast.rename(columns={'Bi-Rads': 'BIRADS', 'File Name': 'image_path', 'Laterality': 'laterality', 'View': 'view', 'ACR': 'ACR(density)'})\n",
    "# convert image path to int and then to string\n",
    "inbreast['image_path'] = inbreast['image_path'].astype(int).astype(str)\n",
    "inbreast['image_path'] = 'INbreast/ALLDICOMs/' + inbreast['image_path']\n",
    "inbreast['patientID'] = inbreast['image_path'].str.extract(r'([0-9]+)')\n",
    "inbreast['patientID'] = inbreast['patientID'].str.replace('INbreast/ALLDICOMs/', '')\n",
    "\n",
    "def update_image_path(base_dir, current_path):\n",
    "    number = current_path.split('/')[-1]\n",
    "    dicom_file = [file for file in os.listdir(base_dir) if file.startswith(number)]\n",
    "    new_path = os.path.join(base_dir, dicom_file[0]) if dicom_file else current_path\n",
    "    return new_path\n",
    "\n",
    "# Update the image_path column\n",
    "base_directory = '../Individual_Original_Datasets/INbreast/AllDICOMs'  # Replace with your actual base directory path\n",
    "inbreast['image_path'] = inbreast['image_path'].apply(lambda x: update_image_path(base_directory, x))\n",
    "inbreast['image_type'] = 'full mammogram image'\n",
    "inbreast['dataset'] = 'inbreast'\n",
    "\n",
    "# remove rows with NaN values in image_path\n",
    "inbreast = inbreast.dropna(subset=['image_path'])\n",
    "\n",
    "# BIRADs to NBM mapping\n",
    "birads_to_nbm = {\n",
    "    '0': 'Normal',\n",
    "    '1': 'Normal',\n",
    "    '2': 'Benign',\n",
    "    '3': 'Benign',\n",
    "    # '4': 'Suspicious B or M',   #(Suspicious Anomaly. Biopsy should be considered)\n",
    "    '4a': 'Suspicious Malignant',\n",
    "    '4b': 'Suspicious Malignant',\n",
    "    '4c': 'Suspicious Malignant',\n",
    "    '5': 'Malignant',\n",
    "    '6': 'Malignant'\n",
    "}\n",
    "\n",
    "dataset = inbreast.copy()\n",
    "\n",
    "# convert the BIRADS column to string\n",
    "dataset['BIRADS'] = dataset['BIRADS'].astype(str)\n",
    "dataset.loc[dataset['dataset'] == 'inbreast', 'classification'] = dataset.loc[dataset['dataset'] == 'inbreast', 'BIRADS'].map(birads_to_nbm)\n",
    "print(dataset.head())\n",
    "\n",
    "density_mapping = {\n",
    "    1.0: 'A',\n",
    "    2.0: 'B',\n",
    "    3.0: 'C',\n",
    "    4.0: 'D'\n",
    "}\n",
    "\n",
    "# Replace the density values in the dataset.csv file by mapping it and saving it to a new column density2\n",
    "dataset.loc[dataset['dataset'] == 'inbreast', 'density2'] = dataset.loc[dataset['dataset'] == 'inbreast', 'density'].map(density_mapping)\n",
    "\n",
    "# replace the values in the classification column where BIRADS is 4\n",
    "dataset.loc[dataset['BIRADS'] == 4, 'classification'] = 'Suspicious Malignant'\n",
    "\n",
    "# drop the column density and ACR(density)\n",
    "dataset = dataset.drop(columns=['density', 'ACR(density)'])\n",
    "\n",
    "# renme the column density2 to density\n",
    "dataset = dataset.rename(columns={'density2': 'density'})\n",
    "\n",
    "# change the name of the column image path to original_image_path\n",
    "dataset = dataset.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "# drop the rows with NaN values in the image_path column\n",
    "dataset = dataset.dropna(subset=['original_image_path'])\n",
    "\n",
    "print(dataset.shape)\n",
    "print(dataset.count())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(dataset)\n",
    "find_missing_and_duplicate_files(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-DDSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe mini_ddsm, which has the columns patientID, image_path, laterality, view, density, classification, image_type, dataset\n",
    "mini_ddsm = pd.read_excel('../Individual_Original_Datasets/Mini-DDSM/DataWMask.xlsx')\n",
    "\n",
    "# rename the columns\n",
    "mini_ddsm = mini_ddsm.rename(columns={'Tumour_Contour': 'ROI_path', 'fullPath': 'image_path', 'Side': 'laterality', 'View': 'view', 'Density': 'density', 'Status': 'classification', 'Age': 'age'})\n",
    "\n",
    "# set the column patientID to the number between the firrst '\\' and 2nd '\\' in the image_path\n",
    "mini_ddsm['patientID'] = mini_ddsm['image_path'].str.extract(r'\\\\([0-9]+)\\\\')\n",
    "mini_ddsm['image_path'] = 'Mini-DDSM/' + mini_ddsm['image_path']\n",
    "# replace the \\ with / in teh image path\n",
    "mini_ddsm['image_path'] = mini_ddsm['image_path'].str.replace('\\\\', '/')\n",
    "\n",
    "df['classification'] = df['classification'].replace('Cancer', 'Malignant')\n",
    "\n",
    "# take only the columns patientID, image_path, laterality, view, density, classification, image_type, dataset\n",
    "mini_ddsm = mini_ddsm[['patientID', 'image_path', 'ROI_path', 'laterality', 'view', 'density', 'classification', 'age']]\n",
    "mini_ddsm['image_type'] = 'full mammogram image'\n",
    "mini_ddsm['dataset'] = 'mini-ddsm'\n",
    "\n",
    "# convert - in the ROI_path to nan\n",
    "mini_ddsm['ROI_path'] = mini_ddsm['ROI_path'].replace('-', np.nan)\n",
    "\n",
    "# replace al \\ with / in the ROI_path column and add Mini-DDSM/ in the beginning\n",
    "mini_ddsm['ROI_path'] = mini_ddsm['ROI_path'].str.replace('\\\\', '/')\n",
    "mini_ddsm['ROI_path'] = 'Mini-DDSM/' + mini_ddsm['ROI_path']\n",
    "\n",
    "mini_ddsm['image_path'] = mini_ddsm['image_path'].str.replace('Mini-DDSM/', '../Individual_Original_Datasets/Mini-DDSM/')\n",
    "\n",
    "# change the name of the column image path to original_image_path\n",
    "mini_ddsm = mini_ddsm.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "# convert the density values to A, B, C, D\n",
    "mini_ddsm['density'] = mini_ddsm['density'].replace(1, 'A')\n",
    "mini_ddsm['density'] = mini_ddsm['density'].replace(2, 'B')\n",
    "mini_ddsm['density'] = mini_ddsm['density'].replace(3, 'C')\n",
    "mini_ddsm['density'] = mini_ddsm['density'].replace(4, 'D')\n",
    "# replace 0 by nan\n",
    "mini_ddsm['density'] = mini_ddsm['density'].replace(0, np.nan)\n",
    "\n",
    "print(mini_ddsm.shape)\n",
    "print(mini_ddsm.count())\n",
    "print(mini_ddsm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_save_images_w_ROI2(data):\n",
    "    j = 0\n",
    "    dataset_name = data['dataset'].iloc[0]  # Extract the dataset name from the first row\n",
    "    print(f\"Processing {dataset_name} dataset...\")\n",
    "    new_dir = f\"Original_Dataset/{dataset_name}\"\n",
    "\n",
    "    data['new_path'] = None\n",
    "    data['ROI_path'] = data['ROI_path'].replace('-', np.nan)\n",
    "    data['ROI_path'] = data['ROI_path'].str.replace('Mini-DDSM', '../Individual_Original_Datasets/Mini-DDSM')\n",
    "    data['original_image_path'] = data['original_image_path'].str.replace('Mini-DDSM', '../Individual_Original_Datasets/Mini-DDSM')\n",
    "\n",
    "    # Create a new directory to save the renamed images\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    print(f\"Saving images to {new_dir}...\")\n",
    "    new_paths = []\n",
    "\n",
    "    for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        old_path = row['original_image_path']\n",
    "        new_filename = f\"{row['dataset']}_{j}.jpg\"\n",
    "        new_path = os.path.join(new_dir, new_filename)\n",
    "        ROI_path = row['ROI_path']\n",
    "        \n",
    "        # Save image based on file type\n",
    "        if old_path.endswith('.dcm'):\n",
    "            # if row has nan in ROI_path, then save the image as it is, else save the image and the ROI image\n",
    "            if pd.isna(row['ROI_path']):\n",
    "                ds = pydicom.dcmread(old_path)\n",
    "                img = ds.pixel_array\n",
    "                plt.imsave(new_path, img, cmap='gray')\n",
    "            elif not pd.isna(row['ROI_path']):\n",
    "                ds = pydicom.dcmread(old_path)\n",
    "                img = ds.pixel_array\n",
    "                plt.imsave(new_path, img, cmap='gray')\n",
    "                \n",
    "                ds = pydicom.dcmread(row['ROI_path'])\n",
    "                img = ds.pixel_array\n",
    "                ROI_path = new_path.replace('.jpg', '_ROI.jpg')\n",
    "                plt.imsave(ROI_path, img, cmap='gray')\n",
    "\n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            data.at[i, 'ROI_path'] = ROI_path\n",
    "            new_paths.append(new_path)\n",
    "            j += 1\n",
    "        elif old_path.endswith('.png') or old_path.endswith('.jpeg') or old_path.endswith('.pgm') or old_path.endswith('.jpg'):\n",
    "            if pd.isna(row['ROI_path']):\n",
    "                img = mpimg.imread(old_path)\n",
    "                plt.imsave(new_path, img, cmap='gray')\n",
    "            elif not pd.isna(row['ROI_path']):\n",
    "                img = mpimg.imread(old_path)\n",
    "                plt.imsave(new_path, img, cmap='gray')\n",
    "                \n",
    "                img = mpimg.imread(row['ROI_path'])\n",
    "                ROI_path = new_path.replace('.jpg', '_ROI.jpg')\n",
    "                plt.imsave(ROI_path, img, cmap='gray')\n",
    "        \n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            data.at[i, 'ROI_path'] = ROI_path\n",
    "            new_paths.append(new_path)\n",
    "            j += 1\n",
    "        else:\n",
    "            # Ensure the new path is unique\n",
    "            while os.path.exists(new_path):\n",
    "                j += 1\n",
    "                new_filename = f\"{row['dataset']}_{j}.jpg\"\n",
    "                new_path = os.path.join(new_dir, new_filename)\n",
    "            \n",
    "            # Update the DataFrame with the new path\n",
    "            data.at[i, 'new_path'] = new_path\n",
    "            new_paths.append(new_path)\n",
    "\n",
    "            # Copy the file to the new path\n",
    "            shutil.copy(old_path, new_path)\n",
    "            j += 1\n",
    "    \n",
    "    print(f\"Total images saved: {j}\")\n",
    "\n",
    "    data['original_image_path'] = data['original_image_path'].replace('../Individual_Original_Datasets/', '', regex=True)\n",
    "    data['ROI_path'] = data['ROI_path'].replace('../Individual_Original_Datasets/', '', regex=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data.to_csv(f\"../{dataset_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images_w_ROI2(mini_ddsm)\n",
    "find_missing_and_duplicate_files(mini_ddsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAU-BCMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kau_bcmd = pd.read_excel('../Individual_Original_Datasets/king-abdulaziz-uni/correctSheetlast.xlsx', sheet_name='correctSheet')\n",
    "\n",
    "# Select relevant columns and rename them\n",
    "kau_bcmd = kau_bcmd[['Percentage of\\n grandular tissue(density)', 'PatientID', 'Patient age ', 'Breast  type', 'Breast view', 'Assesment', 'Image path']]\n",
    "kau_bcmd = kau_bcmd.rename(columns={'Percentage of\\n grandular tissue(density)': 'density', 'PatientID': 'patientID', 'Patient age ': 'age', 'Breast  type': 'laterality', 'Breast view': 'view', 'Assesment': 'BIRADS', 'Image path': 'image_path'})\n",
    "\n",
    "# Update image_path\n",
    "kau_bcmd['image_path'] = 'king-abdulaziz-uni/' + kau_bcmd['image_path'].str.extract(r'(/.+)')\n",
    "\n",
    "# Extract age from 'age' column\n",
    "kau_bcmd['age'] = kau_bcmd['age'].astype(str).str.extract(r'(\\d+)')\n",
    "\n",
    "# Process BIRADS column\n",
    "kau_bcmd['BIRADS'] = kau_bcmd['BIRADS'].str.split().str[1]\n",
    "kau_bcmd['BIRADS'] = kau_bcmd['BIRADS'].replace('nan', np.nan)\n",
    "\n",
    "# Convert BIRADS to float, then handle NaNs and convert to int\n",
    "kau_bcmd['BIRADS'] = pd.to_numeric(kau_bcmd['BIRADS'], errors='coerce')  # Coerce errors to NaN\n",
    "kau_bcmd['BIRADS'] = kau_bcmd['BIRADS'].fillna(-1).astype(int)  # Fill NaNs with -1 and convert to int\n",
    "kau_bcmd['image_type'] = 'full mammogram image'\n",
    "kau_bcmd['dataset'] = 'kau-bcmd'\n",
    "\n",
    "# BIRADs to NBM mapping\n",
    "birads_to_nbm = {\n",
    "    1.0: 'Normal',\n",
    "    2.0: 'Benign',\n",
    "    3.0: 'Benign',\n",
    "    4.0: 'Suspicious Malignant',  # (Suspicious Malignant)\n",
    "    5.0: 'Malignant',\n",
    "    6.0: 'Malignant'\n",
    "}\n",
    "\n",
    "# Map BIRADS to classification for 'kau-bcmd' dataset\n",
    "kau_bcmd.loc[kau_bcmd['dataset'] == 'kau-bcmd', 'classification'] = kau_bcmd.loc[kau_bcmd['dataset'] == 'kau-bcmd', 'BIRADS'].map(birads_to_nbm)\n",
    "\n",
    "density_mapping = {\n",
    "    '0%-25%': 'A',\n",
    "    '26%-50%': 'B',\n",
    "    '51%-75%': 'C',\n",
    "    '>75%': 'D'\n",
    "}\n",
    "\n",
    "# Map density values for 'kau-bcmd' dataset\n",
    "kau_bcmd.loc[kau_bcmd['dataset'] == 'kau-bcmd', 'density'] = kau_bcmd.loc[kau_bcmd['dataset'] == 'kau-bcmd', 'density'].map(density_mapping)\n",
    "\n",
    "# replace the .dcm in all image_path with .jpg\n",
    "kau_bcmd['image_path'] = kau_bcmd['image_path'].str.replace('.dcm', '.jpg')\n",
    "kau_bcmd['image_path'] = kau_bcmd['image_path'].str.replace('king-abdulaziz-uni/', '../Individual_Original_Datasets/king-abdulaziz-uni/')\n",
    "\n",
    "# Remove rows with NaN values in image_path\n",
    "kau_bcmd = kau_bcmd.dropna(subset=['image_path'])\n",
    "\n",
    "# drop the images with duplicate image_path\n",
    "kau_bcmd = kau_bcmd.drop_duplicates(subset=['image_path'])\n",
    "\n",
    "print(kau_bcmd.shape)\n",
    "print(kau_bcmd.count())\n",
    "\n",
    "# remove these entries from the dataframe\n",
    "kau_bcmd = kau_bcmd[kau_bcmd['image_path'].apply(os.path.exists)]\n",
    "\n",
    "# rename the column image path to original_image_path\n",
    "kau_bcmd = kau_bcmd.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "print(kau_bcmd.shape)\n",
    "print(kau_bcmd.count())\n",
    "print(kau_bcmd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(kau_bcmd)\n",
    "find_missing_and_duplicate_files(kau_bcmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess inbreast\n",
    "cmmd = pd.read_excel('../Individual_Original_Datasets/CMMD/CMMD_clinicaldata_revision.xlsx')\n",
    "\n",
    "cmmd = cmmd.drop(columns=['number'])\n",
    "# replace the name of the Leftright column with Laterality, and Id1 wiht PatientID\n",
    "cmmd = cmmd.rename(columns={'LeftRight': 'laterality', 'ID1': 'patientID', 'Age': 'age'})\n",
    "\n",
    "# match the patientID in cmmd with Subject ID column in 'metainbreast.csv' file, and get the file location \n",
    "# from there and store it in the column 'image_path' by adding this column\n",
    "metadata = pd.read_csv('../Individual_Original_Datasets/CMMD/metadata.csv')\n",
    "metadata = metadata.rename(columns={'Subject ID': 'patientID', 'File Location': 'image_path'})\n",
    "\n",
    "# remove the .\\ in the image_path and replace it with CMMD instead \n",
    "metadata['image_path'] = metadata['image_path'].str.replace('.\\\\', 'CMMD/')\n",
    "\n",
    "# replace all the '\\' with '/' in the image_path\n",
    "metadata['image_path'] = metadata['image_path'].str.replace('\\\\', '/')\n",
    "\n",
    "# merge the two dataframes on the patientID column. total number of entries should be equal to those in metadata\n",
    "cmmd = pd.merge(cmmd, metadata, on=['patientID'])\n",
    "cmmd = cmmd[['patientID', 'abnormality', 'classification', 'image_path', 'laterality', 'age', 'subtype']]\n",
    "\n",
    "# remove rows with NaN values in image_path\n",
    "cmmd = cmmd.dropna(subset=['image_path'])\n",
    "cmmd['dataset'] = 'cmmd'\n",
    "cmmd['image_type'] = 'full mammogram image'\n",
    "\n",
    "cmmd['image_path'] = cmmd['image_path'].str.replace('CMMD/CMMD/', '../Individual_Original_Datasets/CMMD/CMMD/')\n",
    "\n",
    "# remove the entries with duplicate image paths and store them in a new dataframe\n",
    "duplicate_images = cmmd[cmmd.duplicated(subset=['image_path'], keep=False)]\n",
    "cmmd = cmmd.drop_duplicates(subset=['image_path'])\n",
    "\n",
    "cmmd11 = cmmd.copy()\n",
    "cmmd11['image_path'] = cmmd11['image_path'] + '/1-1.dcm'\n",
    "# checl of the image path exists. if not, remove the entry\n",
    "cmmd11 = cmmd11[cmmd11['image_path'].apply(os.path.exists)]\n",
    "cmmd11['view'] = 'CC'\n",
    "\n",
    "cmmd12 = cmmd.copy()\n",
    "cmmd12['image_path'] = cmmd12['image_path'] + '/1-2.dcm'\n",
    "cmmd12 = cmmd12[cmmd12['image_path'].apply(os.path.exists)]\n",
    "cmmd12['view'] = 'MLO'\n",
    "\n",
    "cmmd13 = cmmd.copy()\n",
    "cmmd13['image_path'] = cmmd13['image_path'] + '/1-3.dcm'\n",
    "cmmd13 = cmmd13[cmmd13['image_path'].apply(os.path.exists)]\n",
    "\n",
    "# replace the D1 in patientID with D1.5 if the path exists\n",
    "cmmd13['patientID'] = cmmd13['patientID'].str.replace('D1', 'D1.5')\n",
    "cmmd11['view'] = 'CC'\n",
    "\n",
    "cmmd14 = cmmd.copy()\n",
    "cmmd14['image_path'] = cmmd14['image_path'] + '/1-4.dcm'\n",
    "cmmd14 = cmmd14[cmmd14['image_path'].apply(os.path.exists)]\n",
    "# replace the D1 in patientID with D1.5 if the path exists\n",
    "cmmd14['patientID'] = cmmd14['patientID'].str.replace('D1', 'D1.5')\n",
    "cmmd14['view'] = 'MLO'\n",
    "\n",
    "# merge the four dataframes\n",
    "cmmd = pd.concat([cmmd11, cmmd12, cmmd13, cmmd14])\n",
    "\n",
    "# Iterate over the dataframe and if 1-3.dcm exists for a particular patientID, then go to the entry containing 1-1.dcm and 1-2.dcm\n",
    "# of the same corresponding patientID and replace the laterality with L for both and view as CC and MLO, and R for 1-3.dcm and 1-4.dcm\n",
    "# of the corresponding patientID and view as CC and MLO respectively\n",
    "for i, row in tqdm(cmmd.iterrows(), total=cmmd.shape[0]):\n",
    "    if '1-3.dcm' in row['image_path']:\n",
    "        patientID = row['patientID']\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-1.dcm')), 'laterality'] = 'L'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-2.dcm')), 'laterality'] = 'L'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-1.dcm')), 'view'] = 'CC'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-2.dcm')), 'view'] = 'MLO'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-3.dcm')), 'laterality'] = 'R'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-3.dcm')), 'view'] = 'CC'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-4.dcm')), 'laterality'] = 'R'\n",
    "        cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['image_path'].str.contains('1-4.dcm')), 'view'] = 'MLO'\n",
    "\n",
    "# remove entries with NaN values in the image_path column\n",
    "cmmd = cmmd.dropna(subset=['image_path'])\n",
    "\n",
    "# change the name of the column image path to original_image_path\n",
    "cmmd = cmmd.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "print(cmmd.shape)\n",
    "print(cmmd.count())\n",
    "print(cmmd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(cmmd)\n",
    "find_missing_and_duplicate_files(cmmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmmd = pd.read_csv('cmmd.csv')\n",
    "\n",
    "print(duplicate_images.shape)\n",
    "print(duplicate_images.columns)\n",
    "\n",
    "# compare the duplicate images with the original images by matching the patientID, laterality\n",
    "# if all 3 match, then put the subtype of the duplicate image in the subtype of the original image\n",
    "# this operation should not afffect the original length of the dataframe\n",
    "for i, row in duplicate_images.iterrows():\n",
    "    patientID = row['patientID']\n",
    "    laterality = row['laterality']\n",
    "    subtype = row['subtype']\n",
    "    classification = row['classification']\n",
    "    cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['laterality'] == laterality), 'subtype'] = subtype\n",
    "    cmmd.loc[(cmmd['patientID'] == patientID) & (cmmd['laterality'] == laterality), 'classification'] = classification\n",
    "\n",
    "print(cmmd.shape)\n",
    "print(cmmd.count())\n",
    "print(cmmd.head())\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "cmmd.to_csv('cmmd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDD-CESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdd_cesm = pd.read_excel('../Individual_Original_Datasets/CDD-CESM/Radiology-manual-annotations.xlsx')\n",
    "\n",
    "# if the BIRADS value is 2$2, replace it with 2\n",
    "cdd_cesm['BIRADS'] = cdd_cesm['BIRADS'].replace('2$2', int(2))\n",
    "\n",
    "# convert all entries to characters\n",
    "cdd_cesm['BIRADS'] = cdd_cesm['BIRADS'].astype(str)\n",
    "\n",
    "# Put nan in all the entries that contain $ as a part in their BIRADS column\n",
    "cdd_cesm['BIRADS'] = cdd_cesm['BIRADS'].replace('.*\\$.*', np.nan, regex=True)\n",
    "\n",
    "# add a columns called dataset and set it to cdd-\n",
    "cdd_cesm['dataset'] = 'cdd-cesm'\n",
    "cdd_cesm.rename(columns={'Image_name': 'image_path', 'Patient_ID': 'patientID', 'Side': 'laterality', 'Breast density (ACR)' : 'density', 'View': 'view', 'Age': 'age', 'Pathology Classification/ Follow up': 'classification'}, inplace=True)\n",
    "\n",
    "cdd_cesm['image_path'] = cdd_cesm['image_path'].str.replace(' ', '')\n",
    "cdd_cesm.rename(columns={'image_path': 'original_image_path', 'new_path': 'image_path'}, inplace=True)\n",
    "\n",
    "# keep only the entries with CESM as the image type\n",
    "cdd_cesm = cdd_cesm[cdd_cesm['Type'] == 'DM']\n",
    "cdd_cesm['original_image_path'] = '../Individual_Original_Datasets/CDD-CESM/Low_Energy_Images/' + cdd_cesm['original_image_path'] + '.jpg'\n",
    "\n",
    "#  remove the column Type, Findings, Tags, Machine\n",
    "cdd_cesm = cdd_cesm.drop(columns=['Type', 'Findings', 'Tags', 'Machine', 'density'])\n",
    "\n",
    "# remove rows with NaN values in image_path\n",
    "cdd_cesm = cdd_cesm.dropna(subset=['original_image_path'])\n",
    "\n",
    "print(cdd_cesm.shape)\n",
    "print(cdd_cesm.count())\n",
    "print(cdd_cesm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(cdd_cesm)\n",
    "find_missing_and_duplicate_files(cdd_cesm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Screening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsna_screening = pd.read_csv('../Individual_Original_Datasets/EMBED/train.csv')\n",
    "rsna_screening = rsna_screening[['patient_id', 'image_id', 'laterality', 'view', 'age', 'BIRADS', 'density']]\n",
    "rsna_screening['age'] = rsna_screening['age'].astype(str).str.extract(r'(\\d+)')\n",
    "\n",
    "# add a column image_path to the inbreastframe, which contains the pth of the image in the fomr patient_id/image_id.dcm\n",
    "rsna_screening['image_path'] = 'EMBED/train_images/' + rsna_screening['patient_id'].astype(str) + '/' + rsna_screening['image_id'].astype(str) + '.dcm'\n",
    "\n",
    "# rename the column patient_id to patientID\n",
    "rsna_screening = rsna_screening.rename(columns={'patient_id': 'patientID'})\n",
    "\n",
    "# remove rows with NaN values in image_path\n",
    "rsna_screening = rsna_screening.dropna(subset=['image_path'])\n",
    "rsna_screening['image_type'] = 'full mammogram image'\n",
    "rsna_screening['dataset'] = 'rsna-screening'\n",
    "\n",
    "# remove the entry with the patientID 2029770528 due to broken entry (image does not exist)\n",
    "rsna_screening = rsna_screening[rsna_screening['image_id'] != 2029770528]\n",
    "rsna_screening = rsna_screening.drop(columns=['image_id'])\n",
    "\n",
    "# BIRADs to NBM mapping\n",
    "birads_to_nbm = {\n",
    "    0.0: 'Normal',\n",
    "    1.0: 'Normal',\n",
    "    2.0: 'Benign',\n",
    "    3.0: 'Benign',\n",
    "    4.0: 'Suspicious Malignant',   #(Suspicious Anomaly. Biopsy should be considered)\n",
    "    5.0: 'Malignant',\n",
    "    6.0: 'Malignant'\n",
    "}\n",
    "\n",
    "rsna_screening.loc[rsna_screening['dataset'] == 'rsna-screening', 'classification'] = rsna_screening.loc[rsna_screening['dataset'] == 'rsna-screening', 'BIRADS'].map(birads_to_nbm)\n",
    "rsna_screening['image_path'] = rsna_screening['image_path'].str.replace('EMBED/train_images/', '../Individual_Original_Datasets/EMBED/train_images/')\n",
    "\n",
    "# remove the entries with NaN values in the image_path column\n",
    "rsna_screening = rsna_screening.dropna(subset=['image_path'])\n",
    "\n",
    "\n",
    "# change the name of the column image path to original_image_path\n",
    "rsna_screening = rsna_screening.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "print(rsna_screening.shape)\n",
    "print(rsna_screening.count())\n",
    "print(rsna_screening.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images(rsna_screening)\n",
    "find_missing_and_duplicate_files(rsna_screening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file Metadata.xlsx, read the file from row number 32, and then store the data in a dataframe, and name the columns\n",
    "dmid = pd.read_excel('../Individual_Original_Datasets/DMID/Metadata.xlsx', skiprows=30)\n",
    "\n",
    "# rename the columns\n",
    "dmid = dmid.rename(columns={'Unnamed: 0': 'patientID', 'Unnamed: 1': 'view', 'Unnamed: 2': 'density', 'Unnamed: 3': 'class', 'Unnamed: 4': 'classification', 'Unnamed: 5': 'x', 'Unnamed: 6': 'y', 'Unnamed: 7': 'radius'})\n",
    "\n",
    "# if the view is MLORT, then view is MLO and laterality is R, if the view is CCRT, then view is CC and laterality is R, if the view is MLOLT, then view is MLO and laterality is L, if the view is CCRT, then view is CC and laterality is L\n",
    "dmid['laterality'] = dmid['view'].str.extract(r'([RL])')\n",
    "dmid['view'] = dmid['view'].str.replace('RT', '')\n",
    "dmid['view'] = dmid['view'].str.replace('LT', '')\n",
    "dmid['image_path'] = 'DMID/DICOM_Images/' + dmid['patientID'].astype(str)+ '.dcm'\n",
    "# convert path to string\n",
    "dmid['image_path'] = dmid['image_path'].astype(str)\n",
    "# remove space from the image_path column\n",
    "dmid['image_path'] = dmid['image_path'].str.replace(' ', '')\n",
    "dmid['image_type'] = 'full mammogram image'\n",
    "dmid['dataset'] = 'dmid'\n",
    "\n",
    "# remove the duplicate patientID entries\n",
    "dmid = dmid.drop_duplicates(subset=['patientID'])\n",
    "\n",
    "# Define the path to the ROI Masks directory\n",
    "roi_masks_dir = '../Individual_Original_Datasets/DMID/ROI_Masks'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "roi_files = os.listdir(roi_masks_dir)\n",
    "\n",
    "# save the files in the ROI_path column by comparing patientID with the file name by removing the extension\n",
    "# add the entry if and only if teh file exists\n",
    "dmid['ROI_path'] = dmid['patientID'].astype(str) + '.tif'\n",
    "# convert path to string\n",
    "dmid['ROI_path'] = dmid['ROI_path'].astype(str)\n",
    "# remove the spcees from the ROI_path column\n",
    "dmid['ROI_path'] = dmid['ROI_path'].str.replace(' ', '')\n",
    "# if the pasth does nt exist set it to nan\n",
    "dmid['ROI_path'] = dmid['ROI_path'].apply(lambda x: os.path.join(roi_masks_dir, x) if x in roi_files else np.nan)\n",
    "\n",
    "# convert N, B, M in classicaistion column to Normal,Benign and Malognantr\n",
    "dmid['classification'] = dmid['classification'].replace('N ', 'Normal')\n",
    "dmid['classification'] = dmid['classification'].replace('B ', 'Benign')\n",
    "dmid['classification'] = dmid['classification'].replace('M ', 'Malignant')\n",
    "dmid['classification'] = dmid['classification'].replace('N', 'Normal')\n",
    "dmid['classification'] = dmid['classification'].replace('B', 'Benign')\n",
    "dmid['classification'] = dmid['classification'].replace('M', 'Malignant')\n",
    "\n",
    "dmid['image_path'] = dmid['image_path'].str.replace('DMID/DICOM_Images/', '../Individual_Original_Datasets/DMID/DICOM_Images/')\n",
    "\n",
    "# remove rows with NaN values in image_path\n",
    "dmid = dmid.dropna(subset=['image_path'])\n",
    "\n",
    "# change the name of the column image path to original_image_path\n",
    "dmid = dmid.rename(columns={'image_path': 'original_image_path'})\n",
    "\n",
    "print(dmid.shape)\n",
    "print(dmid.count())\n",
    "print(dmid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_image(old_path, new_path, ROI_path=None):\n",
    "    # Read the original image\n",
    "    if old_path.endswith('.dcm'):\n",
    "        ds = pydicom.dcmread(old_path)\n",
    "        img = ds.pixel_array\n",
    "    else:\n",
    "        img = plt.imread(old_path)\n",
    "    \n",
    "    # Convert RGBA to RGB if necessary\n",
    "    if img.ndim == 3 and img.shape[2] == 4:  # RGBA image\n",
    "        img_rgb = Image.fromarray(img).convert('RGB')\n",
    "        img_rgb.save(new_path)\n",
    "    else:\n",
    "        plt.imsave(new_path, img, cmap='gray' if img.ndim == 2 else None)\n",
    "    \n",
    "    # Process and save ROI if provided\n",
    "    if ROI_path:\n",
    "        tif_image = tifffile.imread(ROI_path)\n",
    "        jpeg_image = Image.fromarray(tif_image)\n",
    "\n",
    "        # Save ROI\n",
    "        ROI_new_path = new_path.replace('.jpg', '_ROI.jpg')\n",
    "        if jpeg_image.mode == 'RGBA':\n",
    "            jpeg_image = jpeg_image.convert('RGB')\n",
    "        jpeg_image.save(ROI_new_path)\n",
    "\n",
    "def rename_and_save_images_w_ROI(data):\n",
    "    j = 0\n",
    "    dataset_name = data['dataset'].iloc[0]  # Extract the dataset name from the first row\n",
    "    print(f\"Processing {dataset_name} dataset...\")\n",
    "    new_dir = f\"Original_Dataset/{dataset_name}\"\n",
    "\n",
    "    data['new_path'] = None\n",
    "    data['original_image_path'] = data['original_image_path'].str.replace('DMID', '../Individual_Original_Datasets/DMID')\n",
    "    data['ROI_path'] = data['ROI_path'].str.replace('DMID', '../Individual_Original_Datasets/DMID')\n",
    "\n",
    "    # Create a new directory to save the renamed images\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    print(f\"Saving images to {new_dir}...\")\n",
    "    new_paths = []\n",
    "\n",
    "    for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        old_path = row['original_image_path']\n",
    "        new_filename = f\"{row['dataset']}_{j}.jpg\"\n",
    "        new_path = os.path.join(new_dir, new_filename)\n",
    "        ROI_path = row['ROI_path'] if not pd.isna(row['ROI_path']) else None\n",
    "        \n",
    "        process_and_save_image(old_path, new_path, ROI_path)\n",
    "        \n",
    "        # Update the DataFrame with the new path\n",
    "        data.at[i, 'new_path'] = new_path\n",
    "        data.at[i, 'ROI_path'] = ROI_path\n",
    "        new_paths.append(new_path)\n",
    "        j += 1\n",
    "    \n",
    "    print(f\"Total images saved: {j}\")\n",
    "\n",
    "    data['original_image_path'] = data['original_image_path'].replace('../Individual_Original_Datasets/', '', regex=True)\n",
    "    data['ROI_path'] = data['original_image_path']\n",
    "    data['ROI_path'] = data['ROI_path'].replace('.jpg', '_ROI.jpg')\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data.to_csv(f\"{dataset_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_save_images_w_ROI(dmid)\n",
    "find_missing_and_duplicate_files(dmid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the ACR Density and BIRADS from the Reports for DMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = 'dmid.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to extract the entire line after BIRADS and ACR for density\n",
    "def extract_birads_and_density(report_text):\n",
    "    # Regular expression to match the entire line after BIRADS:\n",
    "    birads_match = re.search(r'BIRADS:\\s*(.*)', report_text, re.IGNORECASE)\n",
    "    # Regular expression to match the entire line after ACR (handles different formats like ACR-A, ACR B, etc.)\n",
    "    density_match = re.search(r'ACR\\s*(.*)', report_text, re.IGNORECASE)\n",
    "    \n",
    "    birads = birads_match.group(1).strip() if birads_match else None\n",
    "    density = density_match.group(1).strip() if density_match else None\n",
    "    \n",
    "    return birads, density\n",
    "\n",
    "df['BIRADS'] = ''\n",
    "df['density'] = ''\n",
    "\n",
    "for i in range(0, 510):\n",
    "    patient_id = df['patientID'][i]\n",
    "    # remove IMG from the patient ID\n",
    "    patient_id = patient_id[3:]\n",
    "    report_filename = f'Reports/Img{patient_id}.txt'\n",
    "    # remove all the spaces from the report_filename\n",
    "    report_filename = report_filename.replace(\" \", \"\")\n",
    "    if os.path.exists(report_filename):\n",
    "        with open(report_filename, 'r') as file:\n",
    "            report_text = file.read()\n",
    "        birads, density = extract_birads_and_density(report_text)\n",
    "        df.at[i, 'BIRADS'] = birads\n",
    "        df.at[i, 'density'] = density\n",
    "    else:\n",
    "        print(f\"Report file not found at the path {report_filename}\")\n",
    "\n",
    "df['density'] = df['density'].str.replace(')', '')\n",
    "df['density'] = df['density'].str.replace('.', '')\n",
    "df['density'] = df['density'].str.replace('-', '')\n",
    "df['density'] = df['density'].str.replace(' ', '')\n",
    "df['density'] = df['density'].str.replace(',', '')\n",
    "\n",
    "df['BIRADS'] = df['BIRADS'].str.replace('4a', '4')\n",
    "df['BIRADS'] = df['BIRADS'].str.replace('4b', '4')\n",
    "df['BIRADS'] = df['BIRADS'].str.replace('4c', '4')\n",
    "df.at[44, 'BIRADS'] = '3'\n",
    "df.at[29, 'BIRADS'] = '4'\n",
    "\n",
    "# Replace all the values in the BIRADS column that are not 1, 2, 3, 4, 5, or 0 with NULL\n",
    "df['BIRADS'] = df['BIRADS'].apply(lambda x: x if x in ['1', '2', '3', '4', '5', '0'] else np.nan)\n",
    "\n",
    "# convert the BIRADS and density columns to string\n",
    "df['BIRADS'] = df['BIRADS'].astype(str)\n",
    "\n",
    "# for those entries where classification is null and BIRADS is 0 or 1, set the classification to Normal, 2 or 3 to Benign, 5 to Malignant, 4 to Suspicious Malignant\n",
    "for i in range(len(df)):\n",
    "    if pd.isnull(df['classification'][i]) and df['BIRADS'][i] == '0' or df['BIRADS'][i] == '1':\n",
    "        df.at[i, 'classification'] = 'Normal'\n",
    "    elif pd.isnull(df['classification'][i]) and df['BIRADS'][i] == '2' or df['BIRADS'][i] == '3':\n",
    "        df.at[i, 'classification'] = 'Benign'\n",
    "    elif pd.isnull(df['classification'][i]) and df['BIRADS'][i] == '4':\n",
    "        df.at[i, 'classification'] = 'Suspicious Malignant'\n",
    "    elif pd.isnull(df['classification'][i]) and df['BIRADS'][i] == '5':\n",
    "        df.at[i, 'classification'] = 'Malignant'\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv('dmid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all the dataset files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes\n",
    "mias = pd.read_csv('mias.csv')\n",
    "inbreast = pd.read_csv('inbreast.csv')\n",
    "mini_ddsm = pd.read_csv('mini-ddsm.csv')\n",
    "kau_bcmd = pd.read_csv('kau-bcmd.csv')\n",
    "cmmd = pd.read_csv('cmmd.csv')\n",
    "cdd_cesm = pd.read_csv('cdd-cesm.csv')  \n",
    "# add image_type as full mammogram image\n",
    "cdd_cesm['image_type'] = 'full mammogram image'\n",
    "rsna_screening = pd.read_csv('rsna-screening.csv')\n",
    "dmid = pd.read_csv('dmid.csv')\n",
    "\n",
    "# concatenate the dataframes\n",
    "dataset = pd.concat([mias, inbreast, mini_ddsm, kau_bcmd, cmmd, cdd_cesm, rsna_screening, dmid])\n",
    "\n",
    "# add a column mask_path by replaceing Original_Dataset with Masks\n",
    "dataset['mask_path'] = dataset['new_path'].str.replace('Original_Dataset', 'Masks')\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "dataset.to_csv('dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
